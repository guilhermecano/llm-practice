{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/.cache/pypoetry/virtualenvs/llm-practice-LFgirlFJ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from textwrap import fill\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    )\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader, PyMuPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:36<00:00, 108.46s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.0001\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Explain the difference between ChatGPT and open source LLMs in a couple of lines.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p>\n",
       "ChatGPT is a proprietary model developed by OpenAI, while open source LLMs are models that are made available for anyone to use, modify, and distribute under an open-source license.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
    "result = llm(\n",
    "    query\n",
    ")\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)9d85c1cec4167ed28b43b04c2/.gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 5.62MB/s]\n",
      "(…)ec4167ed28b43b04c2/1_Pooling/config.json: 100%|██████████| 191/191 [00:00<00:00, 697kB/s]\n",
      "(…)4d9ec9d85c1cec4167ed28b43b04c2/README.md: 100%|██████████| 67.9k/67.9k [00:00<00:00, 606kB/s]\n",
      "(…)9ec9d85c1cec4167ed28b43b04c2/config.json: 100%|██████████| 619/619 [00:00<00:00, 4.69MB/s]\n",
      "model.safetensors: 100%|██████████| 670M/670M [00:37<00:00, 18.1MB/s] \n",
      "(…)85c1cec4167ed28b43b04c2/onnx/config.json: 100%|██████████| 632/632 [00:00<00:00, 4.74MB/s]\n",
      "model.onnx: 100%|██████████| 1.34G/1.34G [01:08<00:00, 19.5MB/s]\n",
      "(…)d28b43b04c2/onnx/special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 901kB/s]\n",
      "(…)1cec4167ed28b43b04c2/onnx/tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 6.02MB/s]\n",
      "(…)7ed28b43b04c2/onnx/tokenizer_config.json: 100%|██████████| 342/342 [00:00<00:00, 2.68MB/s]\n",
      "(…)9d85c1cec4167ed28b43b04c2/onnx/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.01MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 670M/670M [00:33<00:00, 19.8MB/s] \n",
      "(…)67ed28b43b04c2/sentence_bert_config.json: 100%|██████████| 57.0/57.0 [00:00<00:00, 317kB/s]\n",
      "(…)4167ed28b43b04c2/special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 895kB/s]\n",
      "(…)9d85c1cec4167ed28b43b04c2/tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 6.13MB/s]\n",
      "(…)ec4167ed28b43b04c2/tokenizer_config.json: 100%|██████████| 342/342 [00:00<00:00, 2.11MB/s]\n",
      "(…)4d9ec9d85c1cec4167ed28b43b04c2/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 69.9MB/s]\n",
      "(…)ec9d85c1cec4167ed28b43b04c2/modules.json: 100%|██████████| 385/385 [00:00<00:00, 4.23MB/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <>\n",
    "Act as a Machine Learning engineer who is teaching high school students.\n",
    "<>\n",
    "\n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Explain what are Deep Neural Networks in 2-3 sentences</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p>Deep neural networks (DNN) are a type of artificial intelligence model that simulates the structure and function of the human brain. They consist of multiple layers of interconnected nodes, or neurons, that process information by passing it through successive layers until an output is generated. DNNs can be trained on large amounts of data to recognize patterns and make predictions, making them useful for tasks such as image recognition, speech recognition, and natural language processing.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
    "result = llm(prompt.format(text=query))\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "pdf_paths = \"/home/guilherme/phd/papers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = PyPDFDirectoryLoader(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pdfs.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts_chunks = text_splitter.split_documents(documents[:10])\n",
    "\n",
    "len(texts_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = documents[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts_chunks, embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <>\n",
    "Use the following information to answer the question at the end.\n",
    "<>\n",
    "\n",
    "{context}\n",
    "\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>What are these papers about?</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p>These papers are about using transformers in reinforcement learning (RL) to improve the performance of RL algorithms. They discuss various approaches that have been developed to enhance the architecture and optimize the trajectories of RL models using transformers. Some examples of these approaches include replacing LSTMs with gated transformer-XL in V-MPO, adding recurrence to the Factorized World Prediction (FWP) network, and combining contrastive loss and a hybrid LSTM-transformer in COBERL. The papers also highlight several applications of transformer-based RL (TRL) in areas such as robotic manipulation, text-based games, and autonomous driving.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are these papers about?\"\n",
    "result_ = qa_chain(\n",
    "    query\n",
    ")\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-practice-LFgirlFJ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
